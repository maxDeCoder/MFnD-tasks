{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95 # Change this value as per requirement\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk, re, sys\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "from einops import rearrange\n",
    "\n",
    "from collections import defaultdict\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = pkl.load(open(\"./weibo/test_id.pickle\", 'rb'))\n",
    "train_id = pkl.load(open(\"./weibo/train_id.pickle\", 'rb'))\n",
    "validate_id = pkl.load(open(\"./weibo/validate_id.pickle\", 'rb'))\n",
    "\n",
    "train_id = pd.DataFrame({\"values\": map(int, train_id.values()), \"tweet id\": map(int, train_id.keys())})\n",
    "test_id = pd.DataFrame({\"values\": map(int, test_id.values()), \"tweet id\": map(int, test_id.keys())})\n",
    "validation_id = pd.DataFrame({\"values\": map(int, validate_id.values()), \"tweet id\": map(int, validate_id.keys())})\n",
    "\n",
    "ids = {\n",
    "    \"train\": train_id.set_index(\"tweet id\")['values'],\n",
    "    \"test\": test_id.set_index(\"tweet id\")['values'],\n",
    "    \"validation\": validation_id.set_index(\"tweet id\")['values']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=\"tweet id|user name|tweet url|user url|publish time| original?|retweet count|comment count|praise count|user id|user authentication type|user fans count|user follow count|user tweet count|publish platform\".split(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str_sst(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for the SST dataset\n",
    "    \"\"\"\n",
    "    string = re.sub(\"[，。 :,.；|-“”——_/nbsp+&;@、《》～（）())#O！：【】]\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def stopwordslist(filepath = './weibo/stop_words.txt'):\n",
    "    stopwords = {}\n",
    "    for line in open(filepath, 'r', encoding='utf-8').readlines():\n",
    "        line = line.strip()\n",
    "        stopwords[line] = 1\n",
    "    #stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
    "    return stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = pd.concat([train_id, validation_id, test_id]).set_index(\"tweet id\")['values']\n",
    "stop_words = stopwordslist()\n",
    "image_paths = {\n",
    "    0: \"./weibo/nonrumor_images/\",\n",
    "    1: \"./weibo/rumor_images/\"\n",
    "}\n",
    "image_filelist = {\n",
    "    0: os.listdir(image_paths[0]),\n",
    "    1: os.listdir(image_paths[1])\n",
    "}\n",
    "\n",
    "def load_tweets(split):\n",
    "    map_id = {}\n",
    "    tweet_data = []\n",
    "    pre_path = \"./weibo/tweets/\"\n",
    "    id = ids[split]\n",
    "    file_list = [(0, pre_path + \"test_nonrumor.txt\"), (1, pre_path + \"test_rumor.txt\"), \\\n",
    "                        (0, pre_path + \"train_nonrumor.txt\"), (1, pre_path + \"train_rumor.txt\")]\n",
    "    \n",
    "    for label, path in file_list:\n",
    "        with open(path, 'r', encoding='utf-8') as input_file:\n",
    "            while True:\n",
    "                try:\n",
    "                    lines = ['', '', '']\n",
    "                    data = {}\n",
    "\n",
    "                    for i in range(len(lines)):\n",
    "                        lines[i]=next(input_file).replace(\"\\n\", \"\")\n",
    "\n",
    "                    l1, l2, l3 = lines\n",
    "                    tweet_id = int(l1.split('|')[0])\n",
    "                    # get tweet details\n",
    "\n",
    "                    data.update(dict([(col, item) for col, item in zip(columns, l1.split(\"|\"))]))\n",
    "\n",
    "                    found = False\n",
    "                    for item in l2.split(\"|\")[:-1]:\n",
    "                        item = item.split(\"/\")[-1]\n",
    "                        if item in image_filelist[label]:\n",
    "                            found = True\n",
    "                            break\n",
    "\n",
    "                    data['image'] = image_paths[label] + item\n",
    "\n",
    "                    l3 = clean_str_sst(l3)\n",
    "                    seg_list = jieba.cut_for_search(l3)\n",
    "                    new_seg_list = []\n",
    "                    for word in seg_list:\n",
    "                        if word not in stop_words:\n",
    "                            new_seg_list.append(word)\n",
    "\n",
    "                    l3 = \" \".join(new_seg_list)\n",
    "\n",
    "                    data['tweet_content'] = l3\n",
    "                    \n",
    "                    # there are more than 10 tokens in the text\n",
    "                    if len(l3) > 10 and tweet_id in id.index:\n",
    "                        event = id[tweet_id]\n",
    "                        if event not in map_id:\n",
    "                            map_id[event] = len(map_id)\n",
    "                            event = map_id[event]\n",
    "                        else:\n",
    "                            event = map_id[event]\n",
    "\n",
    "                        data['event'] = event\n",
    "                        data['label'] = label\n",
    "                        tweet_data.append(data)\n",
    "                        \n",
    "                except StopIteration:\n",
    "                    print(\"End of file reached\")\n",
    "                    break\n",
    "\n",
    "                # except Exception as e:\n",
    "                #     print(e)\n",
    "                #     # break\n",
    "\n",
    "    return pd.DataFrame.from_records(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_tweets(\"train\")[['tweet id', 'tweet_content', 'image', 'event', 'label']]\n",
    "test_dataset = load_tweets('test')[['tweet id', 'tweet_content', 'image', 'event', 'label']]\n",
    "\n",
    "# max number of events are 10, but test set contain 14 unique events, we will the everything that is > 9\n",
    "test_dataset = test_dataset[test_dataset['event'] <= 9]\n",
    "\n",
    "validation_dataset = load_tweets('validation')[['tweet id', 'tweet_content', 'image', 'event', 'label']]\n",
    "all_text = pd.concat([train_dataset['tweet_content'] + test_dataset['tweet_content'] + validation_dataset['tweet_content']]).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_roots = {\n",
    "    1: \"./weibo/rumor_images\",\n",
    "    0: \"./weibo/nonrumor_images\",\n",
    "}\n",
    "def load_image(path):\n",
    "    def center_crop(image, dim):\n",
    "        width, height = image.size\n",
    "        new_width, new_height = dim, dim\n",
    "\n",
    "        left = (width - new_width)/2\n",
    "        top = (height - new_height)/2\n",
    "        right = (width + new_width)/2\n",
    "        bottom = (height + new_height)/2\n",
    "\n",
    "        # Crop the center of the image\n",
    "        image = image.crop((left, top, right, bottom))\n",
    "        return image\n",
    "\n",
    "    image = Image.open(path)\n",
    "    if len(np.array(image).shape) != 3:\n",
    "        new_image = Image.new('RGB', image.size)\n",
    "        new_image.paste(image)\n",
    "        image = new_image\n",
    "\n",
    "    image = image.resize((256, 256))\n",
    "    image = center_crop(image, 224)\n",
    "    image = np.array(image, dtype=np.float32)/255\n",
    "\n",
    "    \n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = \"./weibo/w2v.pickle\"\n",
    "w2v = pkl.load(open(embedding_path, 'rb'), encoding='latin1')\n",
    "vocab = list(w2v.keys())\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SEQ_LENGTH = 28\n",
    "VECTOR_DIM = 32\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return tokenizer(str(sentence), max_length=SEQ_LENGTH, padding='max_length', truncation=True)['input_ids']\n",
    "\n",
    "def get_matrix(sentence):\n",
    "    vectors = np.zeros((SEQ_LENGTH, VECTOR_DIM), dtype=np.float32)\n",
    "    for i, word in enumerate(sentence[:SEQ_LENGTH]):\n",
    "        vectors[i, :] = w2v[word]\n",
    "\n",
    "    return vectors\n",
    "\n",
    "def preprocess_image(text, image, event, label):\n",
    "    image = load_image(image.numpy().decode('utf-8'))\n",
    "    \n",
    "    return text, image, event, label\n",
    "\n",
    "def dict_map(text, image, event, label):\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"image\": image\n",
    "    }, event, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_train_dataset = train_dataset[train_dataset['label'] == 1]\n",
    "# neg_train_dataset = train_dataset[train_dataset['label'] == 0]\n",
    "\n",
    "# pos_train_texts = np.array(pos_train_dataset['tweet_content'].map(tokenize).to_list(), dtype=np.float32)\n",
    "# pos_train_images = pos_train_dataset['image'].to_list()\n",
    "# pos_train_events = pos_train_dataset['event'].to_list()\n",
    "# pos_train_labels = pos_train_dataset['label'].to_list()\n",
    "# pos_train_ds = (tf.data.Dataset.from_tensor_slices((pos_train_texts, pos_train_images, pos_train_events, pos_train_labels))\n",
    "#             .map(lambda text, image, event, label: tf.py_function(preprocess_image, [text, image, event, label], [tf.float32, tf.float32, tf.int32, tf.int32]))\n",
    "#             .map(dict_map)\n",
    "#             .shuffle(1000)\n",
    "#             .batch(BATCH_SIZE)\n",
    "#             .prefetch(tf.data.AUTOTUNE)\n",
    "#             )\n",
    "\n",
    "# neg_train_texts = np.array(neg_train_dataset['tweet_content'].map(tokenize).to_list(), dtype=np.float32)\n",
    "# neg_train_images = neg_train_dataset['image'].to_list()\n",
    "# neg_train_events = neg_train_dataset['event'].to_list()\n",
    "# neg_train_labels = neg_train_dataset['label'].to_list()\n",
    "# neg_train_ds = (tf.data.Dataset.from_tensor_slices((neg_train_texts, neg_train_images, neg_train_events, neg_train_labels))\n",
    "#             .map(lambda text, image, event, label: tf.py_function(preprocess_image, [text, image, event, label], [tf.float32, tf.float32, tf.int32, tf.int32]))\n",
    "#             .map(dict_map)\n",
    "#             .shuffle(1000)\n",
    "#             .batch(BATCH_SIZE)\n",
    "#             .prefetch(tf.data.AUTOTUNE)\n",
    "#             # .repeat()\n",
    "#             )\n",
    "\n",
    "\n",
    "# pos_validation_dataset = validation_dataset[validation_dataset['label'] == 1]\n",
    "# neg_validation_dataset = validation_dataset[validation_dataset['label'] == 0]\n",
    "\n",
    "# pos_validation_texts = np.array(pos_validation_dataset['tweet_content'].map(tokenize).to_list(), dtype=np.float32)\n",
    "# pos_validation_images = pos_validation_dataset['image'].to_list()\n",
    "# pos_validation_events = pos_validation_dataset['event'].to_list()\n",
    "# pos_validation_labels = pos_validation_dataset['label'].to_list()\n",
    "# pos_validation_ds = (tf.data.Dataset.from_tensor_slices((pos_validation_texts, pos_validation_images, pos_validation_events, pos_validation_labels))\n",
    "#             .map(lambda text, image, event, label: tf.py_function(preprocess_image, [text, image, event, label], [tf.float32, tf.float32, tf.int32, tf.int32]))\n",
    "#             .map(dict_map)\n",
    "#             .shuffle(1000)\n",
    "#             .batch(BATCH_SIZE)\n",
    "#             .prefetch(tf.data.AUTOTUNE)\n",
    "#             )\n",
    "\n",
    "# neg_validation_texts = np.array(neg_validation_dataset['tweet_content'].map(tokenize).to_list(), dtype=np.float32)\n",
    "# neg_validation_images = neg_validation_dataset['image'].to_list()\n",
    "# neg_validation_events = neg_validation_dataset['event'].to_list()\n",
    "# neg_validation_labels = neg_validation_dataset['label'].to_list()\n",
    "# neg_validation_ds = (tf.data.Dataset.from_tensor_slices((neg_validation_texts, neg_validation_images, neg_validation_events, neg_validation_labels))\n",
    "#             .map(lambda text, image, event, label: tf.py_function(preprocess_image, [text, image, event, label], [tf.float32, tf.float32, tf.int32, tf.int32]))\n",
    "#             .map(dict_map)\n",
    "#             .shuffle(1000)\n",
    "#             .batch(BATCH_SIZE)\n",
    "#             .prefetch(tf.data.AUTOTUNE)\n",
    "#             # .repeat()\n",
    "#             )\n",
    "\n",
    "\n",
    "# pos_test_dataset = test_dataset[test_dataset['label'] == 1]\n",
    "# neg_test_dataset = test_dataset[test_dataset['label'] == 0]\n",
    "\n",
    "# pos_test_texts = np.array(pos_test_dataset['tweet_content'].map(tokenize).to_list(), dtype=np.float32)\n",
    "# pos_test_images = pos_test_dataset['image'].to_list()\n",
    "# pos_test_events = pos_test_dataset['event'].to_list()\n",
    "# pos_test_labels = pos_test_dataset['label'].to_list()\n",
    "# pos_test_ds = (tf.data.Dataset.from_tensor_slices((pos_test_texts, pos_test_images, pos_test_events, pos_test_labels))\n",
    "#             .map(lambda text, image, event, label: tf.py_function(preprocess_image, [text, image, event, label], [tf.float32, tf.float32, tf.int32, tf.int32]))\n",
    "#             .map(dict_map)\n",
    "#             .shuffle(1000)\n",
    "#             .batch(BATCH_SIZE)\n",
    "#             .prefetch(tf.data.AUTOTUNE)\n",
    "#             )\n",
    "\n",
    "# neg_test_texts = np.array(neg_test_dataset['tweet_content'].map(tokenize).to_list(), dtype=np.float32)\n",
    "# neg_test_images = neg_test_dataset['image'].to_list()\n",
    "# neg_test_events = neg_test_dataset['event'].to_list()\n",
    "# neg_test_labels = neg_test_dataset['label'].to_list()\n",
    "# neg_test_ds = (tf.data.Dataset.from_tensor_slices((neg_test_texts, neg_test_images, neg_test_events, neg_test_labels))\n",
    "#             .map(lambda text, image, event, label: tf.py_function(preprocess_image, [text, image, event, label], [tf.float32, tf.float32, tf.int32, tf.int32]))\n",
    "#             .map(dict_map)\n",
    "#             .shuffle(1000)\n",
    "#             .batch(BATCH_SIZE)\n",
    "#             .prefetch(tf.data.AUTOTUNE)\n",
    "#             # .repeat()\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_texts = np.array((train_dataset['tweet_content'].map(get_matrix).to_list()))\n",
    "train_texts = np.array(train_dataset['tweet_content'].map(tokenize).to_list(), dtype=np.float32)\n",
    "\n",
    "train_images = train_dataset['image'].to_list()\n",
    "train_events = train_dataset['event'].to_list()\n",
    "train_labels = train_dataset['label'].to_list()\n",
    "train_ds = (tf.data.Dataset.from_tensor_slices((train_texts, train_images, train_events, train_labels))\n",
    "            .map(lambda text, image, event, label: tf.py_function(preprocess_image, [text, image, event, label], [tf.float32, tf.float32, tf.int32, tf.int32]))\n",
    "            .map(dict_map)\n",
    "            .shuffle(1000)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "            )\n",
    "\n",
    "\n",
    "# test_texts = np.array((test_dataset['tweet_content'].map(get_matrix).to_list()))\n",
    "test_texts = np.array(test_dataset['tweet_content'].map(tokenize).to_list(), dtype=np.float32)\n",
    "test_images = test_dataset['image'].to_list()\n",
    "test_events = test_dataset['event'].to_list()\n",
    "test_labels = test_dataset['label'].to_list()\n",
    "test_ds = (tf.data.Dataset.from_tensor_slices((test_texts, test_images, test_events, test_labels))          \n",
    "            .map(lambda text, image, event, label: tf.py_function(preprocess_image, [text, image, event, label], [tf.float32, tf.float32, tf.int32, tf.int32]))\n",
    "            .map(dict_map)\n",
    "            .shuffle(1000)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "            )\n",
    "\n",
    "# validation_texts = np.array((validation_dataset['tweet_content'].map(get_matrix).to_list()))\n",
    "validation_texts = np.array(validation_dataset['tweet_content'].map(tokenize).to_list(), dtype=np.float32)\n",
    "validation_images = validation_dataset['image'].to_list()\n",
    "validation_events = validation_dataset['event'].to_list()\n",
    "validation_labels = validation_dataset['label'].to_list()\n",
    "validation_ds = (tf.data.Dataset.from_tensor_slices((validation_texts, validation_images, validation_events, validation_labels))\n",
    "            .map(lambda text, image, event, label: tf.py_function(preprocess_image, [text, image, event, label], [tf.float32, tf.float32, tf.int32, tf.int32]))\n",
    "            .map(dict_map)\n",
    "            .shuffle(1000)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIMS = 32\n",
    "NUM_FILTERS = 20\n",
    "WINDOW_SIZE = [1, 2, 3, 4]\n",
    "EPOCHS = 10\n",
    "\n",
    "p = np.linspace(0, 1, 10)\n",
    "alpha = 10\n",
    "beta = 0.75\n",
    "lmd = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_extractor():\n",
    "    vgg19 = keras.applications.VGG19(\n",
    "        include_top=False,\n",
    "        input_shape=(224,224,3)\n",
    "    )\n",
    "    vgg19.layers[0]._name = \"image\"\n",
    "    vgg19.trainable = False\n",
    "\n",
    "    text_input = layers.Input((SEQ_LENGTH,), name='text')\n",
    "    text_embeddings = layers.Embedding(vocab_size, HIDDEN_DIMS)(text_input)\n",
    "\n",
    "    # image feature extractor\n",
    "    image_features = layers.Flatten()(vgg19.output)\n",
    "    image_features = layers.Dense(HIDDEN_DIMS, activation='leaky_relu')(image_features)\n",
    "\n",
    "    # text feature extractor\n",
    "    convs = [layers.Conv1D(NUM_FILTERS, k)(text_embeddings) for k in WINDOW_SIZE]\n",
    "    pools = [layers.MaxPooling1D(C.shape[1])(C) for C in convs]\n",
    "\n",
    "    text_cnn = layers.Concatenate()(pools)\n",
    "    text_cnn = layers.Dense(HIDDEN_DIMS, activation='leaky_relu')(text_cnn)\n",
    "\n",
    "    feature_extractor = Model(inputs=[vgg19.input, text_input], outputs=[text_cnn[:, 0, :], image_features])\n",
    "    return feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image augumentation\n",
    "augumentor = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomTranslation(height_factor=[-.2, .2], width_factor=[-.2, .2])\n",
    "])\n",
    "\n",
    "def augument(image, n_aug=4):\n",
    "    return [augumentor(image) for _ in range(n_aug)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label aware contrastive loss for pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1: Training similar to CLiP\n",
    "* Calculate vector similarities between text and images, \n",
    "* if the corresponding label is 0, then similarity is expected to be 1\n",
    "* if the cooresponding label is 1, then similarity is expected to be -1\n",
    "* based on the instructions provided in the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = load_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_batched(A, B):\n",
    "    # Normalize vectors\n",
    "    A_normalized = tf.nn.l2_normalize(A, axis=1)\n",
    "    B_normalized = tf.nn.l2_normalize(B, axis=1)\n",
    "\n",
    "    # Compute dot product\n",
    "    dot_product = tf.matmul(A_normalized, B_normalized, transpose_b=True)\n",
    "\n",
    "    return dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, E, Y = train_ds.as_numpy_iterator().next()\n",
    "images = X['image']\n",
    "texts = X['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augumented_images = augument(images)\n",
    "augumented_texts = [texts] * len(augumented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feature_vector, text_feature_vector, labels = [], [], []\n",
    "\n",
    "for i, t in zip(augumented_images, augumented_texts):\n",
    "    encoded_i, encoded_t = feature_extractor((i, t))\n",
    "    image_feature_vector.append(encoded_i)\n",
    "    text_feature_vector.append(encoded_t)\n",
    "    labels.append(Y)\n",
    "\n",
    "image_feature_vector = tf.concat(image_feature_vector, axis=0)\n",
    "text_feature_vector = tf.concat(text_feature_vector, axis=0)\n",
    "labels = tf.concat(labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feature_vector.shape, text_feature_vector.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity_batched(image_feature_vector, text_feature_vector)\n",
    "similarities_logits = tf.linalg.tensor_diag_part(similarities)\n",
    "loss = tf.losses.mean_squared_error(labels, similarities_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.AdamW(learning_rate=0.001)\n",
    "n_augumentations = 4\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(\"EPOCH \", i+1)\n",
    "\n",
    "    train_ds_iter = train_ds.as_numpy_iterator()\n",
    "\n",
    "    train_progbar = Progbar(len(train_ds))\n",
    "\n",
    "    loss_metric = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    for step in range(len(train_ds)):\n",
    "        X, E, Y = train_ds_iter.next()\n",
    "        images = X['image']\n",
    "        texts = X['text']\n",
    "\n",
    "        # Get image augumentations\n",
    "        augumented_images = augument(images, n_aug=n_augumentations)\n",
    "        augumented_texts = [texts] * n_augumentations\n",
    "        \n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            image_feature_vector, text_feature_vector = [], []\n",
    "\n",
    "            for i, t in zip(augumented_images, augumented_texts):\n",
    "                encoded_i, encoded_t = feature_extractor((i, t))\n",
    "                image_feature_vector.append(encoded_i)\n",
    "                text_feature_vector.append(encoded_t)\n",
    "\n",
    "            image_feature_vector = tf.concat(image_feature_vector, axis=0)\n",
    "            text_feature_vector = tf.concat(text_feature_vector, axis=0)\n",
    "\n",
    "            similarities = cosine_similarity_batched(image_feature_vector, text_feature_vector)\n",
    "            similarities_logits = tf.linalg.tensor_diag_part(similarities)\n",
    "            loss = tf.losses.mean_squared_error(labels, similarities_logits)\n",
    "            # labels = (Y.astype(bool) == False).astype(int)\n",
    "            # labels = tf.concat([labels] * n_augumentations, axis=0)\n",
    "            # labels = tf.where(tf.equal(labels, 0), -1 * tf.ones_like(labels), labels)\n",
    "            # targets = tf.linalg.diag(labels)\n",
    "\n",
    "            # loss = tf.reduce_mean(tf.losses.mean_squared_error(targets, similarities))\n",
    "\n",
    "        grads = tape.gradient(loss, feature_extractor.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, feature_extractor.trainable_variables))\n",
    "\n",
    "        loss_value = loss_metric(loss)\n",
    "\n",
    "        train_progbar.update(step+1, [\n",
    "            ('loss', loss_value),\n",
    "        ])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2: Label Aware Supervised Contrastive Loss\n",
    "* based on the code that was provided in the assignment document and the Supervised Contrastive Loss paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = load_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        # Compute logits\n",
    "        logits = tf.divide(\n",
    "            tf.matmul(\n",
    "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.07\n",
    "\n",
    "optimizer = keras.optimizers.AdamW(learning_rate=0.001)\n",
    "scl = SupervisedContrastiveLoss(temperature=temperature)\n",
    "\n",
    "for i in range(5): # 5 epochs\n",
    "    print(\"EPOCH \", i+1)\n",
    "\n",
    "    train_ds_iter = train_ds.as_numpy_iterator()\n",
    "\n",
    "    train_progbar = Progbar(len(train_ds))\n",
    "\n",
    "    loss_metric = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "\n",
    "    for step in range(len(train_ds)):\n",
    "        X, E, Y = train_ds_iter.next()\n",
    "        images = X['image']\n",
    "        texts = X['text']\n",
    "\n",
    "        positive_idx = tf.where(Y == 1)[:, 0]\n",
    "        negative_idx = tf.where(Y == 0)[:, 0]\n",
    "        augumented_pos_images = augument(images)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            processed = tf.convert_to_tensor([feature_extractor((aug_imgs, aug_texts)) for aug_imgs, aug_texts in zip(augumented_pos_images, [texts]*len(augumented_pos_images))])\n",
    "            encoded_images = processed[:, 0]\n",
    "            encoded_texts = processed[:, 1]\n",
    "\n",
    "            positive_label_img_features = rearrange(tf.gather(encoded_images, positive_idx, axis=1), 'a b d -> (a b) d')\n",
    "            positive_label_txt_features = rearrange(tf.gather(encoded_texts, positive_idx, axis=1), 'a b d -> (a b) d')\n",
    "            positive_data_features = tf.concat([positive_label_img_features, positive_label_txt_features], axis=0)\n",
    "\n",
    "            negative_label_img_features = rearrange(tf.gather(encoded_images, negative_idx, axis=1), 'a b d -> (a b) d')\n",
    "            negative_label_txt_features = rearrange(tf.gather(encoded_texts, negative_idx, axis=1), 'a b d -> (a b) d')\n",
    "            negative_data_features = tf.concat([negative_label_img_features, negative_label_txt_features], axis=0)\n",
    "\n",
    "            all_features = tf.concat([positive_data_features, negative_data_features], axis=0)\n",
    "            all_labels = tf.concat([tf.ones(positive_data_features.shape[0]), tf.zeros(negative_data_features.shape[0])], axis=0)\n",
    "            loss = scl(all_labels, all_features)\n",
    "\n",
    "\n",
    "        grads = tape.gradient(loss, feature_extractor.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, feature_extractor.trainable_variables))\n",
    "\n",
    "        loss_value = loss_metric(loss)\n",
    "\n",
    "        train_progbar.update(step+1, [\n",
    "            ('loss', loss_value),\n",
    "        ])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the full network with fake news detector and event classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversal(keras.layers.Layer):\n",
    "    def __init__(self, λ=1, **kwargs):\n",
    "        super(GradientReversal, self).__init__(**kwargs)\n",
    "        self.λ = λ\n",
    "\n",
    "    @staticmethod\n",
    "    @tf.custom_gradient\n",
    "    def reverse_gradient(x, λ):\n",
    "        return tf.identity(x), lambda dy: (-dy, None)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.reverse_gradient(x, self.λ)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(GradientReversal, self).get_config() | {'λ': self.λ}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This 'feature_extractor' comes from either approach 1 or approach 2\n",
    "features = layers.Concatenate(name=\"multi_modal_feature\")(feature_extractor.outputs)\n",
    "\n",
    "# Fake news detector\n",
    "features = layers.Dropout(0.2)(features)\n",
    "predictor = layers.Dense(2, activation=\"softmax\", name='prediction')(features)\n",
    "\n",
    "# Event Discriminator\n",
    "grd_r = GradientReversal(λ=lmd)(features)\n",
    "event_discriminator = layers.Dense(HIDDEN_DIMS, activation='leaky_relu')(grd_r)\n",
    "event_discriminator = layers.Dropout(0.2)(event_discriminator)\n",
    "event_discriminator = layers.Dense(HIDDEN_DIMS, activation='leaky_relu')(event_discriminator)\n",
    "event_discriminator = layers.Dense(10, activation='softmax', name='event_discriminator')(event_discriminator)\n",
    "\n",
    "model = Model(inputs=feature_extractor.inputs, outputs=[predictor, event_discriminator])\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin=1\n",
    "\n",
    "binary_ce = keras.losses.BinaryCrossentropy()\n",
    "categorical_ce = keras.losses.CategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.AdamW(learning_rate=0.001)\n",
    "\n",
    "for i in range(20): # 20 epochs\n",
    "    print(\"EPOCH \", i+1)\n",
    "    train_ds_iter = train_ds.as_numpy_iterator()\n",
    "    validation_ds_iter = validation_ds.as_numpy_iterator()\n",
    "\n",
    "    train_progbar = Progbar(len(train_ds))\n",
    "    \n",
    "    loss_D_metric = keras.metrics.Mean(name=\"detector_loss\")\n",
    "    loss_E_metric = keras.metrics.Mean(name=\"event_loss\")\n",
    "    loss_final_metric = keras.metrics.Mean(name=\"final_loss\")\n",
    "\n",
    "    fake_news_accuracy = keras.metrics.CategoricalAccuracy (name=\"fake_news_accuracy\")\n",
    "    event_accuracy = keras.metrics.CategoricalAccuracy (name=\"event_accuracy\")\n",
    "\n",
    "    for step in range(len(train_ds)):\n",
    "    # for step in range(1):\n",
    "        X, E, Y = train_ds_iter.next()\n",
    "        Y = keras.utils.to_categorical(Y, num_classes=2)\n",
    "        E = keras.utils.to_categorical(E, num_classes=10)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred, event = model(X)\n",
    "\n",
    "            Ld = binary_ce(Y, pred)\n",
    "            Le = categorical_ce(E, event)\n",
    "\n",
    "            final_loss = Ld / (lmd*Le)\n",
    "\n",
    "        grads = tape.gradient(final_loss, model.trainable_variables)\n",
    "\n",
    "        # calculate Learning rate\n",
    "        # optimizer.learning_rate = lr_schedule(i, optimizer.learning_rate)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # metrics\n",
    "        Acc_F = fake_news_accuracy(Y, pred)\n",
    "        Acc_E = event_accuracy(E, event)\n",
    "        m_Ld = loss_D_metric(Ld)\n",
    "        m_Le = loss_E_metric(Le)\n",
    "        m_L_final = loss_final_metric(final_loss)\n",
    "\n",
    "        train_progbar.update(step+1, [\n",
    "            ('lr', optimizer.learning_rate),\n",
    "            ('detector loss', m_Ld),\n",
    "            ('event loss', m_Le),\n",
    "            ('final loss', m_L_final),\n",
    "            ('fake news accuracy', Acc_F),\n",
    "            ('event accuracy', Acc_E),\n",
    "        ])\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    loss_D_metric = keras.metrics.Mean(name=\"detector_loss\")\n",
    "    loss_E_metric = keras.metrics.Mean(name=\"event_loss\")\n",
    "    loss_final_metric = keras.metrics.Mean(name=\"final_loss\")\n",
    "    \n",
    "    fake_news_accuracy = keras.metrics.CategoricalAccuracy (name=\"fake_news_accuracy\")\n",
    "    event_accuracy = keras.metrics.CategoricalAccuracy (name=\"event_accuracy\")\n",
    "\n",
    "    validation_progbar = Progbar(len(validation_ds))\n",
    "\n",
    "    for step in range(len(validation_ds)):\n",
    "        X, E, Y = validation_ds_iter.next()\n",
    "        Y = keras.utils.to_categorical(Y, num_classes=2)\n",
    "        E = keras.utils.to_categorical(E, num_classes=10)\n",
    "\n",
    "        pred, event = model(X)\n",
    "\n",
    "        Ld = binary_ce(Y, pred)\n",
    "        Le = categorical_ce(E, event)\n",
    "\n",
    "        final_loss = Ld / (lmd*Le)\n",
    "\n",
    "        # metrics\n",
    "        Acc_F = fake_news_accuracy(Y, pred)\n",
    "        Acc_E = event_accuracy(E, event)\n",
    "        m_Ld = loss_D_metric(Ld)\n",
    "        m_Le = loss_E_metric(Le)\n",
    "        m_L_final = loss_final_metric(final_loss)\n",
    "\n",
    "        validation_progbar.update(step+1, [\n",
    "            # ('lr', optimizer.learning_rate),\n",
    "            ('val detector loss', m_Ld),\n",
    "            ('val event loss', m_Le),\n",
    "            ('val final loss', m_L_final),\n",
    "            ('val fake news accuracy', Acc_F),\n",
    "            ('val event accuracy', Acc_E),\n",
    "        ])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./models/task2-approach2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions \n",
    "binary_ce = keras.losses.BinaryCrossentropy()\n",
    "categorical_ce = keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "test_ds_iter = test_ds.as_numpy_iterator()\n",
    "\n",
    "loss_D_metric = keras.metrics.Mean(name=\"detector_loss\")\n",
    "loss_E_metric = keras.metrics.Mean(name=\"event_loss\")\n",
    "loss_final_metric = keras.metrics.Mean(name=\"final_loss\")\n",
    "\n",
    "fake_news_accuracy = keras.metrics.CategoricalAccuracy(name=\"fake_news_accuracy\")\n",
    "event_accuracy = keras.metrics.CategoricalAccuracy(name=\"event_accuracy\")\n",
    "\n",
    "test_progbar = Progbar(len(test_ds))\n",
    "\n",
    "for step in range(len(test_ds)):\n",
    "    X, E, Y = test_ds_iter.next()\n",
    "    E = keras.utils.to_categorical(E, num_classes=10)\n",
    "\n",
    "    pred, event = model(X)\n",
    "\n",
    "    Ld = binary_ce(Y, pred[:, 0])\n",
    "    Le = categorical_ce(E, event)\n",
    "\n",
    "    final_loss = (lmd * Le) - Ld\n",
    "\n",
    "    # metrics\n",
    "    Acc_F = fake_news_accuracy(Y, pred[:, 0])\n",
    "    Acc_E = event_accuracy(E, event)\n",
    "    m_Ld = loss_D_metric(Ld)\n",
    "    m_Le = loss_E_metric(Le)\n",
    "    m_L_final = loss_final_metric(final_loss)\n",
    "\n",
    "    test_progbar.update(step+1, [\n",
    "        ('test detector loss', m_Ld),\n",
    "        ('test event loss', m_Le),\n",
    "        ('test final loss', m_L_final),\n",
    "        ('test fake news accuracy', Acc_F),\n",
    "        ('test event accuracy', Acc_E),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
