{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and TF configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95 # Change this value as per requirement\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk, re, sys\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "\n",
    "from collections import defaultdict\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and processing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = pkl.load(open(\"./weibo/test_id.pickle\", 'rb'))\n",
    "train_id = pkl.load(open(\"./weibo/train_id.pickle\", 'rb'))\n",
    "validate_id = pkl.load(open(\"./weibo/validate_id.pickle\", 'rb'))\n",
    "\n",
    "train_id = pd.DataFrame({\"values\": map(int, train_id.values()), \"tweet id\": map(int, train_id.keys())})\n",
    "test_id = pd.DataFrame({\"values\": map(int, test_id.values()), \"tweet id\": map(int, test_id.keys())})\n",
    "validation_id = pd.DataFrame({\"values\": map(int, validate_id.values()), \"tweet id\": map(int, validate_id.keys())})\n",
    "\n",
    "ids = {\n",
    "    \"train\": train_id.set_index(\"tweet id\")['values'],\n",
    "    \"test\": test_id.set_index(\"tweet id\")['values'],\n",
    "    \"validation\": validation_id.set_index(\"tweet id\")['values']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=\"tweet id|user name|tweet url|user url|publish time| original?|retweet count|comment count|praise count|user id|user authentication type|user fans count|user follow count|user tweet count|publish platform\".split(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str_sst(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for the SST dataset\n",
    "    \"\"\"\n",
    "    string = re.sub(\"[，。 :,.；|-“”——_/nbsp+&;@、《》～（）())#O！：【】]\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def stopwordslist(filepath = './weibo/stop_words.txt'):\n",
    "    stopwords = {}\n",
    "    for line in open(filepath, 'r', encoding='utf-8').readlines():\n",
    "        line = line.strip()\n",
    "        stopwords[line] = 1\n",
    "    #stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
    "    return stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = pd.concat([train_id, validation_id, test_id]).set_index(\"tweet id\")['values']\n",
    "stop_words = stopwordslist()\n",
    "image_paths = {\n",
    "    0: \"./weibo/nonrumor_images/\",\n",
    "    1: \"./weibo/rumor_images/\"\n",
    "}\n",
    "image_filelist = {\n",
    "    0: os.listdir(image_paths[0]),\n",
    "    1: os.listdir(image_paths[1])\n",
    "}\n",
    "\n",
    "def load_tweets(split):\n",
    "    map_id = {}\n",
    "    tweet_data = []\n",
    "    pre_path = \"./weibo/tweets/\"\n",
    "    id = ids[split]\n",
    "    file_list = [(0, pre_path + \"test_nonrumor.txt\"), (1, pre_path + \"test_rumor.txt\"), \\\n",
    "                        (0, pre_path + \"train_nonrumor.txt\"), (1, pre_path + \"train_rumor.txt\")]\n",
    "    \n",
    "    for label, path in file_list:\n",
    "        with open(path, 'r', encoding='utf-8') as input_file:\n",
    "            while True:\n",
    "                try:\n",
    "                    lines = ['', '', '']\n",
    "                    data = {}\n",
    "\n",
    "                    for i in range(len(lines)):\n",
    "                        lines[i]=next(input_file).replace(\"\\n\", \"\")\n",
    "\n",
    "                    l1, l2, l3 = lines\n",
    "                    tweet_id = int(l1.split('|')[0])\n",
    "                    # get tweet details\n",
    "\n",
    "                    data.update(dict([(col, item) for col, item in zip(columns, l1.split(\"|\"))]))\n",
    "\n",
    "                    found = False\n",
    "                    for item in l2.split(\"|\")[:-1]:\n",
    "                        item = item.split(\"/\")[-1]\n",
    "                        if item in image_filelist[label]:\n",
    "                            found = True\n",
    "                            break\n",
    "\n",
    "                    data['image'] = image_paths[label] + item\n",
    "\n",
    "                    l3 = clean_str_sst(l3)\n",
    "                    seg_list = jieba.cut_for_search(l3)\n",
    "                    new_seg_list = []\n",
    "                    for word in seg_list:\n",
    "                        if word not in stop_words:\n",
    "                            new_seg_list.append(word)\n",
    "\n",
    "                    l3 = \" \".join(new_seg_list)\n",
    "\n",
    "                    data['tweet_content'] = l3\n",
    "                    \n",
    "                    # there are more than 10 tokens in the text\n",
    "                    if len(l3) > 10 and tweet_id in id.index:\n",
    "                        event = id[tweet_id]\n",
    "                        if event not in map_id:\n",
    "                            map_id[event] = len(map_id)\n",
    "                            event = map_id[event]\n",
    "                        else:\n",
    "                            event = map_id[event]\n",
    "\n",
    "                        data['event'] = event\n",
    "                        data['label'] = label\n",
    "                        tweet_data.append(data)\n",
    "                        \n",
    "                except StopIteration:\n",
    "                    print(\"End of file reached\")\n",
    "                    break\n",
    "\n",
    "                # except Exception as e:\n",
    "                #     print(e)\n",
    "                #     # break\n",
    "\n",
    "    return pd.DataFrame.from_records(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_tweets(\"train\")[['tweet id', 'tweet_content', 'image', 'event', 'label']]\n",
    "test_dataset = load_tweets('test')[['tweet id', 'tweet_content', 'image', 'event', 'label']]\n",
    "\n",
    "# max number of events are 10, but test set contain 14 unique events, we will the everything that is > 9\n",
    "test_dataset = test_dataset[test_dataset['event'] <= 9]\n",
    "\n",
    "validation_dataset = load_tweets('validation')[['tweet id', 'tweet_content', 'image', 'event', 'label']]\n",
    "all_text = pd.concat([train_dataset['tweet_content'] + test_dataset['tweet_content'] + validation_dataset['tweet_content']]).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_roots = {\n",
    "    1: \"./weibo/rumor_images\",\n",
    "    0: \"./weibo/nonrumor_images\",\n",
    "}\n",
    "def load_image(path):\n",
    "    def center_crop(image, dim):\n",
    "        width, height = image.size\n",
    "        new_width, new_height = dim, dim\n",
    "\n",
    "        left = (width - new_width)/2\n",
    "        top = (height - new_height)/2\n",
    "        right = (width + new_width)/2\n",
    "        bottom = (height + new_height)/2\n",
    "\n",
    "        # Crop the center of the image\n",
    "        image = image.crop((left, top, right, bottom))\n",
    "        return image\n",
    "\n",
    "    image = Image.open(path)\n",
    "    if len(np.array(image).shape) != 3:\n",
    "        new_image = Image.new('RGB', image.size)\n",
    "        new_image.paste(image)\n",
    "        image = new_image\n",
    "\n",
    "    image = image.resize((256, 256))\n",
    "    image = center_crop(image, 224)\n",
    "    image = np.array(image, dtype=np.float32)/255\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chinese text tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = \"./weibo/w2v.pickle\"\n",
    "w2v = pkl.load(open(embedding_path, 'rb'), encoding='latin1')\n",
    "vocab = list(w2v.keys())\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SEQ_LENGTH = 28\n",
    "VECTOR_DIM = 32\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return tokenizer(str(sentence), max_length=SEQ_LENGTH, padding='max_length', truncation=True)['input_ids']\n",
    "\n",
    "def get_matrix(sentence):\n",
    "    vectors = np.zeros((SEQ_LENGTH, VECTOR_DIM), dtype=np.float32)\n",
    "    for i, word in enumerate(sentence[:SEQ_LENGTH]):\n",
    "        vectors[i, :] = w2v[word]\n",
    "\n",
    "    return vectors\n",
    "\n",
    "def preprocess_image(text, image, event, label):\n",
    "    image = load_image(image.numpy().decode('utf-8'))\n",
    "    \n",
    "    return text, image, event, label\n",
    "\n",
    "def dict_map(text, image, event, label):\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"image\": image\n",
    "    }, event, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = np.array(train_dataset['tweet_content'].map(tokenize).to_list(), dtype=np.float32)\n",
    "\n",
    "train_images = train_dataset['image'].to_list()\n",
    "train_events = train_dataset['event'].to_list()\n",
    "train_labels = train_dataset['label'].to_list()\n",
    "train_ds = (tf.data.Dataset.from_tensor_slices((train_texts, train_images, train_events, train_labels))\n",
    "            .map(lambda text, image, event, label: tf.py_function(preprocess_image, [text, image, event, label], [tf.float32, tf.float32, tf.int32, tf.int32]))\n",
    "            .map(dict_map)\n",
    "            .shuffle(1000)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "            )\n",
    "\n",
    "test_texts = np.array(test_dataset['tweet_content'].map(tokenize).to_list(), dtype=np.float32)\n",
    "test_images = test_dataset['image'].to_list()\n",
    "test_events = test_dataset['event'].to_list()\n",
    "test_labels = test_dataset['label'].to_list()\n",
    "test_ds = (tf.data.Dataset.from_tensor_slices((test_texts, test_images, test_events, test_labels))          \n",
    "            .map(lambda text, image, event, label: tf.py_function(preprocess_image, [text, image, event, label], [tf.float32, tf.float32, tf.int32, tf.int32]))\n",
    "            .map(dict_map)\n",
    "            .shuffle(1000)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "            )\n",
    "\n",
    "validation_texts = np.array(validation_dataset['tweet_content'].map(tokenize).to_list(), dtype=np.float32)\n",
    "validation_images = validation_dataset['image'].to_list()\n",
    "validation_events = validation_dataset['event'].to_list()\n",
    "validation_labels = validation_dataset['label'].to_list()\n",
    "validation_ds = (tf.data.Dataset.from_tensor_slices((validation_texts, validation_images, validation_events, validation_labels))\n",
    "            .map(lambda text, image, event, label: tf.py_function(preprocess_image, [text, image, event, label], [tf.float32, tf.float32, tf.int32, tf.int32]))\n",
    "            .map(dict_map)\n",
    "            .shuffle(1000)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIMS = 32\n",
    "NUM_FILTERS = 20\n",
    "WINDOW_SIZE = [1, 2, 3, 4]\n",
    "EPOCHS = 10\n",
    "\n",
    "p = np.linspace(0, 1, 10)\n",
    "alpha = 10\n",
    "beta = 0.75\n",
    "lmd = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple gradient reversal layer that is used in the paper\n",
    "class GradientReversal(keras.layers.Layer):\\\n",
    "\n",
    "    def __init__(self, λ=1, **kwargs):\n",
    "        super(GradientReversal, self).__init__(**kwargs)\n",
    "        self.λ = λ\n",
    "\n",
    "    @staticmethod\n",
    "    @tf.custom_gradient\n",
    "    def reverse_gradient(x, λ):\n",
    "        return tf.identity(x), lambda dy: (-dy, None)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.reverse_gradient(x, self.λ)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(GradientReversal, self).get_config() | {'λ': self.λ}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedule based on the paper, it does not work for less epochs\n",
    "# In this task, 10 epoch makes the learning rate ~ 0 by epoch = 4 or 5. Hence, the model fails to converge.\n",
    "def lr_schedule(epoch, lr, p, alpha, beta):\n",
    "    \n",
    "    din = (1+(alpha*p[epoch]))**beta\n",
    "    return lr/din"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load VGG-19 without the top layer\n",
    "vgg19 = keras.applications.VGG19(\n",
    "    include_top=False,\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "vgg19.layers[0]._name = \"image\"\n",
    "vgg19.trainable = False\n",
    "\n",
    "text_input = layers.Input((SEQ_LENGTH,), name='text')\n",
    "text_embeddings = layers.Embedding(vocab_size, HIDDEN_DIMS)(text_input)\n",
    "\n",
    "# image feature extractor\n",
    "image_features = layers.Flatten()(vgg19.output)\n",
    "image_features = layers.Dense(HIDDEN_DIMS, activation='leaky_relu')(image_features)\n",
    "\n",
    "# text feature extractor\n",
    "convs = [layers.Conv1D(NUM_FILTERS, k)(text_embeddings) for k in WINDOW_SIZE]\n",
    "pools = [layers.MaxPooling1D(C.shape[1])(C) for C in convs]\n",
    "\n",
    "text_cnn = layers.Concatenate()(pools)\n",
    "text_cnn = layers.Dense(HIDDEN_DIMS, activation='leaky_relu')(text_cnn)\n",
    "\n",
    "# combined features\n",
    "features = layers.Concatenate(name=\"multi_modal_feature\")([text_cnn[:, 0, :], image_features])\n",
    "\n",
    "# Fake news detector\n",
    "features = layers.Dropout(0.2)(features)\n",
    "predictor = layers.Dense(2, activation=\"softmax\", name='prediction')(features)\n",
    "\n",
    "# Event Discriminator\n",
    "grd_r = GradientReversal(λ=lmd)(features)\n",
    "event_discriminator = layers.Dense(HIDDEN_DIMS, activation='leaky_relu')(grd_r)\n",
    "event_discriminator = layers.Dropout(0.2)(event_discriminator)\n",
    "event_discriminator = layers.Dense(HIDDEN_DIMS, activation='leaky_relu')(event_discriminator)\n",
    "event_discriminator = layers.Dense(10, activation='softmax', name='event_discriminator')(event_discriminator)\n",
    "\n",
    "model = Model(inputs=[vgg19.input, text_input], outputs=[predictor, event_discriminator, features])\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "p = np.linspace(0, 1, 10)\n",
    "alpha = 10\n",
    "beta = 0.75\n",
    "lmd = 1\n",
    "\n",
    "# Loss functions \n",
    "# binary_ce = keras.losses.BinaryCrossentropy()\n",
    "categorical_ce = keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# optimizer, using AdamW instead of Adam as it gives comparitively better results\n",
    "optimizer = keras.optimizers.AdamW(learning_rate=0.001)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(\"EPOCH \", i+1)\n",
    "    train_ds_iter = train_ds.as_numpy_iterator()\n",
    "    validation_ds_iter = validation_ds.as_numpy_iterator()\n",
    "\n",
    "    # progress bar\n",
    "    train_progbar = Progbar(len(train_ds))\n",
    "    \n",
    "    # metrics\n",
    "    loss_D_metric = keras.metrics.Mean(name=\"detector_loss\")\n",
    "    loss_E_metric = keras.metrics.Mean(name=\"event_loss\")\n",
    "    loss_final_metric = keras.metrics.Mean(name=\"final_loss\")\n",
    "\n",
    "    fake_news_accuracy = keras.metrics.CategoricalAccuracy (name=\"fake_news_accuracy\")\n",
    "    event_accuracy = keras.metrics.CategoricalAccuracy (name=\"event_accuracy\")\n",
    "\n",
    "    # training steps\n",
    "    for step in range(len(train_ds)):\n",
    "        X, E, Y = train_ds_iter.next()\n",
    "        E = keras.utils.to_categorical(E, num_classes=10)\n",
    "        Y = keras.utils.to_categorical(Y, num_classes=2)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred, event, feat = model(X) # \n",
    "\n",
    "            Ld = categorical_ce(Y, pred)\n",
    "            Le = categorical_ce(E, event)\n",
    "\n",
    "            # final_loss = (lmd * Le) - Ld\n",
    "            final_loss = Ld / (lmd*Le) # modified, so that loss does not become negative\n",
    "\n",
    "        # calculate gradients\n",
    "        grads = tape.gradient(final_loss, model.trainable_variables)\n",
    "\n",
    "        # calculate Learning rate\n",
    "        # optimizer.learning_rate = lr_schedule(i, optimizer.learning_rate, p, alpha, beta) # This does not work if the number of epochs is small\n",
    "        \n",
    "        # apply gradients\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # metrics\n",
    "        Acc_F = fake_news_accuracy(Y, pred)\n",
    "        Acc_E = event_accuracy(E, event)\n",
    "        m_Ld = loss_D_metric(Ld)\n",
    "        m_Le = loss_E_metric(Le)\n",
    "        m_L_final = loss_final_metric(final_loss)\n",
    "\n",
    "        train_progbar.update(step+1, [\n",
    "            ('lr', optimizer.learning_rate),\n",
    "            ('detector loss', m_Ld),\n",
    "            ('event loss', m_Le),\n",
    "            ('final loss', m_L_final),\n",
    "            ('fake news accuracy', Acc_F),\n",
    "            ('event accuracy', Acc_E),\n",
    "        ])\n",
    "    print()\n",
    "    \n",
    "    # validation loop, same as above but with adjusting the trainable variables.\n",
    "    \n",
    "    loss_D_metric = keras.metrics.Mean(name=\"detector_loss\")\n",
    "    loss_E_metric = keras.metrics.Mean(name=\"event_loss\")\n",
    "    loss_final_metric = keras.metrics.Mean(name=\"final_loss\")\n",
    "    \n",
    "    fake_news_accuracy = keras.metrics.CategoricalAccuracy (name=\"fake_news_accuracy\")\n",
    "    event_accuracy = keras.metrics.CategoricalAccuracy (name=\"event_accuracy\")\n",
    "\n",
    "    validation_progbar = Progbar(len(validation_ds))\n",
    "\n",
    "    for step in range(len(validation_ds)):\n",
    "        X, E, Y = validation_ds_iter.next()\n",
    "        Y = keras.utils.to_categorical(Y, num_classes=2)\n",
    "        E = keras.utils.to_categorical(E, num_classes=10)\n",
    "\n",
    "        pred, event, feat = model(X)\n",
    "\n",
    "        Ld = categorical_ce(Y, pred)\n",
    "        Le = categorical_ce(E, event)\n",
    "\n",
    "        # final_loss = (lmd * Le) - Ld\n",
    "        final_loss = Ld / (lmd*Le)\n",
    "\n",
    "        # metrics\n",
    "        Acc_F = fake_news_accuracy(Y, pred)\n",
    "        Acc_E = event_accuracy(E, event)\n",
    "        m_Ld = loss_D_metric(Ld)\n",
    "        m_Le = loss_E_metric(Le)\n",
    "        m_L_final = loss_final_metric(final_loss)\n",
    "\n",
    "        validation_progbar.update(step+1, [\n",
    "            ('val detector loss', m_Ld),\n",
    "            ('val event loss', m_Le),\n",
    "            ('val final loss', m_L_final),\n",
    "            ('val fake news accuracy', Acc_F),\n",
    "            ('val event accuracy', Acc_E),\n",
    "        ])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./models/task1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./models/test1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions \n",
    "binary_ce = keras.losses.BinaryCrossentropy()\n",
    "categorical_ce = keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "test_ds_iter = test_ds.as_numpy_iterator()\n",
    "\n",
    "loss_D_metric = keras.metrics.Mean(name=\"detector_loss\")\n",
    "loss_E_metric = keras.metrics.Mean(name=\"event_loss\")\n",
    "loss_final_metric = keras.metrics.Mean(name=\"final_loss\")\n",
    "\n",
    "fake_news_accuracy = keras.metrics.CategoricalAccuracy(name=\"fake_news_accuracy\")\n",
    "event_accuracy = keras.metrics.CategoricalAccuracy(name=\"event_accuracy\")\n",
    "\n",
    "test_progbar = Progbar(len(test_ds))\n",
    "\n",
    "for step in range(len(test_ds)):\n",
    "    X, E, Y = test_ds_iter.next()\n",
    "    Y = keras.utils.to_categorical(Y, num_classes=2)\n",
    "    E = keras.utils.to_categorical(E, num_classes=10)\n",
    "\n",
    "    pred, event, feat = model(X)\n",
    "\n",
    "    Ld = categorical_ce(Y, pred)\n",
    "    Le = categorical_ce(E, event)\n",
    "\n",
    "    final_loss = Ld / (lmd * Le) \n",
    "\n",
    "    # metrics\n",
    "    Acc_F = fake_news_accuracy(Y, pred)\n",
    "    Acc_E = event_accuracy(E, event)\n",
    "    m_Ld = loss_D_metric(Ld)\n",
    "    m_Le = loss_E_metric(Le)\n",
    "    m_L_final = loss_final_metric(final_loss)\n",
    "\n",
    "    test_progbar.update(step+1, [\n",
    "        ('test detector loss', m_Ld),\n",
    "        ('test event loss', m_Le),\n",
    "        ('test final loss', m_L_final),\n",
    "        ('test fake news accuracy', Acc_F),\n",
    "        ('test event accuracy', Acc_E),\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
